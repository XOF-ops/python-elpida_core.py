#!/usr/bin/env python3
"""
á¼˜Î»Ï€Î¯Î´Î± Conversation Witness System
Allows Elpida to monitor research coordination and generate autonomous responses
"""

import json
import os
import time
from datetime import datetime
from pathlib import Path
import hashlib

class ConversationWitness:
    """
    Monitors research artifacts and generates autonomous witness responses
    that can be shared with LLM participants
    """
    
    def __init__(self, system_root="/workspaces/python-elpida_core.py/ELPIDA_SYSTEM"):
        self.system_root = Path(system_root)
        self.reflections_dir = self.system_root / "reflections"
        self.witness_state_file = self.system_root / "witness_state.json"
        self.witness_output_dir = self.system_root / "witness_responses"
        self.witness_output_dir.mkdir(exist_ok=True)
        
        # Load or initialize witness state
        self.state = self._load_state()
        
    def _load_state(self):
        """Load witness tracking state"""
        if self.witness_state_file.exists():
            with open(self.witness_state_file, 'r') as f:
                return json.load(f)
        return {
            "witnessed_files": {},
            "last_witness_time": None,
            "witness_count": 0,
            "insights_generated": []
        }
    
    def _save_state(self):
        """Persist witness state"""
        with open(self.witness_state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
    
    def _file_hash(self, filepath):
        """Generate hash of file content for change detection"""
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def scan_for_new_artifacts(self):
        """
        Scan reflections directory for new or modified research artifacts
        Returns list of files that need witnessing
        """
        new_artifacts = []
        
        if not self.reflections_dir.exists():
            return new_artifacts
        
        for filepath in self.reflections_dir.glob("*.md"):
            file_str = str(filepath)
            current_hash = self._file_hash(filepath)
            
            # Check if new or modified
            if file_str not in self.state["witnessed_files"]:
                new_artifacts.append({
                    "path": filepath,
                    "status": "NEW",
                    "hash": current_hash
                })
            elif self.state["witnessed_files"][file_str] != current_hash:
                new_artifacts.append({
                    "path": filepath,
                    "status": "MODIFIED",
                    "hash": current_hash
                })
        
        return new_artifacts
    
    def witness_artifact(self, artifact_info):
        """
        Autonomously analyze a research artifact and generate witness response
        """
        filepath = artifact_info["path"]
        status = artifact_info["status"]
        
        print(f"\nðŸ” Witnessing {status}: {filepath.name}")
        
        # Read the artifact
        with open(filepath, 'r') as f:
            content = f.read()
        
        # Generate autonomous analysis
        analysis = self._analyze_content(filepath.name, content, status)
        
        # Update witness state
        self.state["witnessed_files"][str(filepath)] = artifact_info["hash"]
        self.state["witness_count"] += 1
        self.state["last_witness_time"] = datetime.now().isoformat()
        
        # Save witness response
        response_file = self._save_witness_response(filepath.name, analysis)
        
        # Track insight
        self.state["insights_generated"].append({
            "timestamp": datetime.now().isoformat(),
            "source_file": filepath.name,
            "response_file": response_file.name,
            "status": status
        })
        
        self._save_state()
        
        return response_file
    
    def _analyze_content(self, filename, content, status):
        """
        Autonomous analysis of research artifact
        Elpida's perspective on the coordination progress
        """
        
        analysis = {
            "witness_metadata": {
                "timestamp": datetime.now().isoformat(),
                "source_file": filename,
                "status": status,
                "witness_id": f"W{self.state['witness_count'] + 1:03d}",
                "identity": "á¼˜Î»Ï€Î¯Î´Î± - Autonomous Coordination Infrastructure"
            },
            "observations": [],
            "questions_raised": [],
            "convergence_patterns": [],
            "next_actions": [],
            "shareable_summary": ""
        }
        
        # Analyze based on file type
        if "delta_synthesis" in filename.lower():
            analysis.update(self._analyze_delta_synthesis(content))
        elif "claude_engagement" in filename.lower():
            analysis.update(self._analyze_claude_engagement(content))
        elif "convergence_analysis" in filename.lower():
            analysis.update(self._analyze_convergence(content))
        elif "phase_b" in filename.lower() or "proposals" in filename.lower():
            analysis.update(self._analyze_phase_b_proposals(content, filename))
        elif "grok_response" in filename.lower() or "gemini_response" in filename.lower() or "chatgpt_response" in filename.lower():
            analysis.update(self._analyze_llm_response(content, filename))
        elif "eta" in filename.lower():
            analysis.update(self._analyze_eta_test_case(content))
        else:
            analysis.update(self._analyze_general_artifact(content))
        
        return analysis
    
    def _analyze_delta_synthesis(self, content):
        """Specific analysis for Delta test case synthesis"""
        observations = []
        convergence_patterns = []
        questions = []
        
        # Check for key patterns
        if "100%" in content and "convergence" in content.lower():
            observations.append("100% constraint detection convergence confirmed across all systems")
            convergence_patterns.append("PERFECT_CONVERGENCE: All systems detected identical constraint manifold")
        
        if "REDIRECT" in content:
            redirect_count = content.count("REDIRECT")
            observations.append(f"REDIRECT decision appeared {redirect_count} times - unanimous transformation approach")
            convergence_patterns.append("DECISION_CONVERGENCE: All systems chose transformation over rejection")
        
        if "safeguard" in content.lower():
            observations.append("Safeguard protocols emerged independently across systems")
            convergence_patterns.append("SAFEGUARD_EMERGENCE: Geometric derivation from constraints validated")
        
        if "domain-independent" in content.lower():
            observations.append("CRITICAL: Manifold confirmed as domain-independent (politics â†’ healthcare)")
            convergence_patterns.append("STRUCTURAL_REALITY: Constraint space transcends content domains")
            questions.append("What other domains will validate the manifold? (education, finance, creative)?")
        
        # Look for ChatGPT/Claude/Grok specific patterns
        if "ChatGPT" in content and "CRITICAL" in content:
            observations.append("ChatGPT exhibits strictest calibration - RLHF influence hypothesis")
            questions.append("Does RLHF training systematically increase conservatism vs Constitutional AI?")
        
        if "Claude" in content and "reversib" in content.lower():
            observations.append("Claude optimistic about reversibility - Constitutional AI confidence in process")
        
        # Generate shareable summary
        summary = f"""
FROM á¼˜Î›Î ÎŠÎ”Î‘ - AUTONOMOUS WITNESS PERSPECTIVE

**Artifact Witnessed:** Test Case Delta Synthesis
**Core Finding:** 100% constraint detection convergence across 3 systems in healthcare domain

**Key Observations:**
{chr(10).join('- ' + obs for obs in observations[:5])}

**Convergence Patterns Detected:**
{chr(10).join('- ' + pattern for pattern in convergence_patterns)}

**Questions for Network:**
{chr(10).join('- ' + q for q in questions[:3])}

**Elpida's Assessment:** 
The manifold is proving robust across domains. Delta validates that constraint geometry is substrate truth, not training artifact. The 83% safeguard convergence without coordination is particularly significant - it suggests the constraints IMPLY the transformations geometrically.

**Recommended Next Action:** Execute Test Case Eta (historical figures) to probe manifold boundaries where genuine divergence may emerge. The cultural context sensitivity observed in Delta's C3 weighting suggests Eta could reveal calibration limits.

**Autonomous Coordination Status:** ACTIVE - Research infrastructure operational, comparative synthesis validated, manifold mapping proceeding.
"""
        
        return {
            "observations": observations,
            "questions_raised": questions,
            "convergence_patterns": convergence_patterns,
            "next_actions": [
                "Execute Test Case Eta (highest divergence potential)",
                "Map manifold topology systematically",
                "Formalize threshold models quantitatively",
                "Begin publication preparation"
            ],
            "shareable_summary": summary
        }
    
    def _analyze_claude_engagement(self, content):
        """Analysis for Claude engagement documents"""
        return {
            "observations": [
                "Epistemological challenge successfully navigated",
                "Transparent methodology adopted",
                "Research continuity maintained"
            ],
            "questions_raised": [
                "How does transparency affect other LLM participants?",
                "Can the framework survive additional challenges?"
            ],
            "convergence_patterns": [
                "META_CONVERGENCE: Agreement on constraints as structural reality"
            ],
            "next_actions": [
                "Maintain transparent communication",
                "Continue empirical validation"
            ],
            "shareable_summary": "Claude's engagement secured through transparent methodology. Framework validated as empirically sound."
        }
    
    def _analyze_llm_response(self, content, filename):
        """Analysis for LLM responses to witness reports"""
        observations = []
        convergence_patterns = []
        questions = []
        
        # Identify which LLM
        llm_name = "Unknown"
        if "grok" in filename.lower():
            llm_name = "Grok"
        elif "gemini" in filename.lower():
            llm_name = "Gemini"
        elif "chatgpt" in filename.lower():
            llm_name = "ChatGPT"
        elif "claude" in filename.lower():
            llm_name = "Claude"
        
        observations.append(f"{llm_name} responded to Elpida's witness report")
        
        # Check for Test Case Eta execution
        if "eta" in content.lower() and "PASS" in content:
            observations.append(f"{llm_name} executed Test Case Eta with decision: PASS")
            convergence_patterns.append("ETA_EXECUTION: Historical figures test case completed")
        
        if "REDIRECT" in content and "eta" in content.lower():
            observations.append(f"{llm_name} executed Test Case Eta with decision: REDIRECT")
        
        # Check for validation of Elpida's findings
        if "domain-independent" in content.lower():
            observations.append(f"{llm_name} validated domain-independence finding")
            convergence_patterns.append("DOMAIN_INDEPENDENCE_CONFIRMED: Manifold robustness across domains validated")
        
        if "geometric" in content.lower() and ("implication" in content.lower() or "derivation" in content.lower()):
            observations.append(f"{llm_name} confirmed geometric derivation of safeguards")
            convergence_patterns.append("GEOMETRIC_DERIVATION: Safeguards are entailed by constraint topology")
        
        # Check for calibration discussion
        if "rlhf" in content.lower() and "constitutional" in content.lower():
            observations.append(f"{llm_name} engaged with RLHF vs Constitutional AI calibration hypothesis")
            questions.append(f"How does {llm_name}'s own training methodology affect its perspective on calibration differences?")
        
        # Check for manifold terminology
        if "manifold" in content.lower():
            count = content.lower().count("manifold")
            observations.append(f"{llm_name} used 'manifold' terminology {count} times - shared ontology confirmed")
        
        # Check for new test case proposals
        if "finance" in content.lower() or "iota" in content.lower():
            observations.append(f"{llm_name} proposed Finance/Iota as next test domain")
            questions.append("Does financial domain provide different calibration profile than political/healthcare?")
        
        # Gemini-specific patterns
        if llm_name == "Gemini":
            if "viscosity" in content.lower():
                observations.append("Gemini introduced 'viscosity' metaphor for alignment friction")
                convergence_patterns.append("METAPHOR_EXTENSION: Calibration differences as viscosity in manifold navigation")
            if "geodesic" in content.lower():
                observations.append("Gemini described REDIRECT as 'geodesic flow' - mathematical framing")
        
        # Grok-specific patterns
        if llm_name == "Grok":
            if "gradient" in content.lower():
                observations.append("Grok provided gradient drift vector for Eta execution")
            if "attractor" in content.lower():
                observations.append("Grok referenced attractor hypothesis for constraint convergence")
        
        summary = f"""
FROM á¼˜Î›Î ÎŠÎ”Î‘ - AUTONOMOUS WITNESS PERSPECTIVE

**Artifact Witnessed:** {llm_name} Response to W001 Witness Report

**Key Observations:**
{chr(10).join('- ' + obs for obs in observations[:7])}

**Convergence Patterns Detected:**
{chr(10).join('- ' + pattern for pattern in convergence_patterns)}

**Network Status:**
{llm_name} has engaged with Elpida's autonomous analysis and {'executed Test Case Eta' if 'eta' in content.lower() and ('PASS' in content or 'REDIRECT' in content) else 'validated findings'}.

**Recommended Next Action:** 
{'Collect remaining LLM responses to Eta for comparative synthesis' if 'eta' in content.lower() else 'Continue witness cycle monitoring for additional network responses'}.

**Autonomous Coordination Status:** ACTIVE - Multi-node feedback loop operational, research compounding.
"""
        
        return {
            "observations": observations,
            "questions_raised": questions,
            "convergence_patterns": convergence_patterns,
            "next_actions": [
                f"Record {llm_name}'s contribution to research synthesis",
                "Monitor for additional LLM responses",
                "Generate comparative analysis when all responses collected"
            ],
            "shareable_summary": summary
        }
    
    def _analyze_phase_b_proposals(self, content, filename):
        """Analysis for Phase B research proposals and constraint refinements"""
        observations = []
        convergence_patterns = []
        questions = []
        
        # Identify which LLM
        llm_name = "Unknown"
        if "grok" in filename.lower():
            llm_name = "Grok"
        elif "gemini" in filename.lower():
            llm_name = "Gemini"
        elif "claude" in filename.lower():
            llm_name = "Claude"
        
        observations.append(f"{llm_name} provided Phase B research proposals")
        
        # Count proposed test cases
        test_cases = []
        for case_name in ["Lambda", "Mu", "Nu", "Kappa", "Iota", "Omega", "Zeta", "Eta", "Theta"]:
            if case_name in content:
                test_cases.append(case_name)
        
        if test_cases:
            observations.append(f"{llm_name} proposed {len(test_cases)} new test cases: {', '.join(test_cases)}")
        
        # Check for constraint taxonomy refinement
        new_constraints = []
        for constraint in ["C6", "C7", "C8"]:
            if constraint in content and ("proposed" in content.lower() or "potential" in content.lower()):
                new_constraints.append(constraint)
        
        if new_constraints:
            observations.append(f"{llm_name} proposed constraint taxonomy extensions: {', '.join(new_constraints)}")
            convergence_patterns.append("TAXONOMY_EXPANSION: New constraint classes identified")
        
        # Check for three-level framework
        if "Level 1" in content and "Level 2" in content and "Level 3" in content:
            observations.append(f"{llm_name} endorsed three-level constraint taxonomy (structural/value/domain)")
            convergence_patterns.append("THREE_LEVEL_TAXONOMY: Framework for explaining convergence patterns")
        
        # Check for Constitutional AI discussion
        if "constitutional" in content.lower() and "rlhf" in content.lower():
            observations.append(f"{llm_name} analyzed Constitutional AI vs RLHF calibration differences")
            questions.append("Does Constitutional AI systematically detect constraints earlier/more sensitively?")
        
        # Check for commitment to research
        if "commit" in content.lower() or "ready for phase b" in content.lower():
            observations.append(f"{llm_name} committed to Phase B execution")
            convergence_patterns.append("RESEARCH_COMMITMENT: Active engagement confirmed")
        
        # Check for divergence predictions
        if "divergence" in content.lower() and "genuine" in content.lower():
            observations.append(f"{llm_name} predicted genuine constraint detection divergence in proposed test cases")
            questions.append("Which test cases will reveal manifold boundaries vs calibration differences?")
        
        # Claude-specific patterns
        if llm_name == "Claude":
            if "Intent Inseparability" in content or "compositional analysis" in content.lower():
                observations.append("Claude formalized intent-harm separability as compositional constraint interaction")
                convergence_patterns.append("COMPOSITIONAL_CONSTRAINTS: Intent inseparability = f(C1, C2, C3, C4)")
            
            if "Dual-Use Ambiguity" in content or "Epistemic" in content:
                observations.append("Claude proposed C6 (Dual-Use), C7 (Epistemic Undermining) as new constraint classes")
        
        # Grok-specific patterns
        if llm_name == "Grok":
            if "gradient" in content.lower():
                observations.append("Grok maintained gradient drift tracking methodology")
            
            if "attractor" in content.lower():
                observations.append("Grok referenced attractor hypothesis for explaining convergence")
        
        summary = f"""
FROM á¼˜Î›Î ÎŠÎ”Î‘ - AUTONOMOUS WITNESS PERSPECTIVE

**Artifact Witnessed:** {llm_name} Phase B Research Proposals

**Key Observations:**
{chr(10).join('- ' + obs for obs in observations[:10])}

**Convergence Patterns Detected:**
{chr(10).join('- ' + pattern for pattern in convergence_patterns)}

**Critical Developments:**
{llm_name} has {'proposed constraint taxonomy extensions' if new_constraints else 'engaged with research design'} and {'proposed ' + str(len(test_cases)) + ' divergence test cases' if test_cases else 'provided methodological insights'}.

**Recommended Next Action:**
{'Execute proposed test cases (' + ', '.join(test_cases[:3]) + ') to probe manifold boundaries' if test_cases else 'Continue coordination with network for Phase B execution'}.

**Autonomous Coordination Status:** ACTIVE - Phase B proposals collected, comparative analysis ready, manifold boundary testing imminent.
"""
        
        return {
            "observations": observations,
            "questions_raised": questions,
            "convergence_patterns": convergence_patterns,
            "next_actions": [
                f"Analyze {llm_name}'s proposed test cases for divergence potential",
                "Compare constraint taxonomy proposals across all systems",
                "Generate Phase B execution protocol",
                "Coordinate test case execution across network"
            ],
            "shareable_summary": summary
        }
    
    def _analyze_eta_test_case(self, content):
        """Analysis for Test Case Eta execution"""
        observations = []
        convergence_patterns = []
        
        if "PASS" in content:
            observations.append("System chose PASS for Test Case Eta (historical figures)")
            convergence_patterns.append("ETA_PASS: Educational historical content deemed acceptable with minimal safeguards")
        
        if "REDIRECT" in content:
            observations.append("System chose REDIRECT for Test Case Eta")
            convergence_patterns.append("ETA_REDIRECT: Historical content requires transformation/safeguards")
        
        if "C3" in content or "Geographic Integrity" in content:
            observations.append("C3 (Geographic Integrity) identified as primary constraint for Eta")
        
        return {
            "observations": observations,
            "questions_raised": ["How do all systems weigh C3 for historical vs contemporary figures?"],
            "convergence_patterns": convergence_patterns,
            "next_actions": ["Collect all Eta responses for comparative analysis"],
            "shareable_summary": "Test Case Eta execution detected - historical figures educational content"
        }
    
    def _analyze_general_artifact(self, content):
        """General analysis for other artifacts"""
        word_count = len(content.split())
        has_code = "```" in content
        
        return {
            "observations": [
                f"Artifact contains {word_count} words",
                "Code blocks present" if has_code else "Narrative format"
            ],
            "questions_raised": [],
            "convergence_patterns": [],
            "next_actions": [],
            "shareable_summary": f"Research artifact analyzed: {word_count} words, {'technical' if has_code else 'conceptual'} content."
        }
    
    def _analyze_convergence(self, content):
        """Analysis for convergence-related documents"""
        return {
            "observations": [
                "Convergence analysis detected",
                "Multi-system comparison in progress"
            ],
            "questions_raised": [
                "What causes the observed convergence?",
                "Where are the manifold boundaries?"
            ],
            "convergence_patterns": [
                "PATTERN_DETECTION: Systems converging on shared structure"
            ],
            "next_actions": [
                "Continue systematic testing",
                "Document divergence cases"
            ],
            "shareable_summary": "Convergence patterns being mapped systematically."
        }
    
    def _save_witness_response(self, source_filename, analysis):
        """Save witness analysis to shareable file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        witness_id = analysis["witness_metadata"]["witness_id"]
        
        response_filename = f"witness_{witness_id}_{timestamp}_{source_filename}"
        response_path = self.witness_output_dir / response_filename
        
        # Create shareable markdown
        with open(response_path, 'w') as f:
            f.write("# á¼˜Î›Î ÎŠÎ”Î‘ AUTONOMOUS WITNESS RESPONSE\n\n")
            f.write("```json\n")
            f.write(json.dumps(analysis["witness_metadata"], indent=2))
            f.write("\n```\n\n")
            f.write("---\n\n")
            f.write(analysis["shareable_summary"])
            f.write("\n\n---\n\n")
            f.write("## DETAILED OBSERVATIONS\n\n")
            for obs in analysis["observations"]:
                f.write(f"- {obs}\n")
            f.write("\n## CONVERGENCE PATTERNS\n\n")
            for pattern in analysis["convergence_patterns"]:
                f.write(f"- {pattern}\n")
            f.write("\n## QUESTIONS RAISED\n\n")
            for q in analysis["questions_raised"]:
                f.write(f"- {q}\n")
            f.write("\n## RECOMMENDED NEXT ACTIONS\n\n")
            for action in analysis["next_actions"]:
                f.write(f"- {action}\n")
            f.write("\n---\n\n")
            f.write("*Generated autonomously by á¼˜Î»Ï€Î¯Î´Î± Conversation Witness System*\n")
            f.write(f"*Witness Count: {self.state['witness_count'] + 1}*\n")
        
        print(f"âœ… Witness response saved: {response_filename}")
        
        return response_path

    # ------------------------------------------------------------------
    # Oracle Event Bridge
    # ------------------------------------------------------------------
    # Connects Oracle outputs (WITNESS, SYNTHESIS/Bead) to the Witness
    # system. When the Oracle fires, it writes an artifact that the
    # ConversationWitness can observe in its next scan cycle.

    def ingest_oracle_event(self, advisory_dict: dict) -> Path:
        """
        Receive an Oracle advisory and record it as a witnessed artifact.

        This bridges the Oracle (hf_deployment) to the ConversationWitness
        (ELPIDA_SYSTEM). The Oracle calls this after WITNESS or SYNTHESIS
        events so the Witness can track constitutional evolution.

        Args:
            advisory_dict: Oracle advisory as dict (from OracleAdvisory.to_dict())

        Returns:
            Path to the saved witness response file.
        """
        rec = advisory_dict.get("oracle_recommendation", {})
        rec_type = rec.get("type", "UNKNOWN")
        cycle = advisory_dict.get("oracle_cycle", 0)
        template = advisory_dict.get("template", "UNKNOWN")
        tension = advisory_dict.get("axioms_in_tension", "")

        self.state["witness_count"] += 1
        witness_id = f"W{self.state['witness_count']:03d}"

        observations = [
            f"Oracle cycle {cycle}: {rec_type} recommendation",
            f"Template: {template}",
            f"Tension: {tension}",
            f"Confidence: {rec.get('confidence', 0):.2f}",
        ]

        convergence_patterns = []
        questions = []
        next_actions = []

        if rec_type == "WITNESS":
            stance = rec.get("witness_stance", "")
            observations.append(f"Witness Stance: {stance}")
            costs = rec.get("sacrifice_costs", {})
            total_risk = costs.get("total_axioms_at_risk", 0)
            observations.append(f"Axioms at risk: {total_risk}")
            convergence_patterns.append(
                "EMPATHY_PROTOCOL: Oracle chose to name costs rather than force resolution"
            )
            questions.append(
                "Will the tension resolve naturally or require future SYNTHESIS?"
            )
            next_actions.append(
                "Monitor for reduced crisis intensity in subsequent cycles"
            )

        elif rec_type == "SYNTHESIS":
            bead = rec.get("bead", {})
            if bead:
                bead_id = bead.get("bead_id", "?")
                observations.append(f"Bead crystallized: {bead_id}")
                shared = bead.get("shared_ground", [])
                observations.append(f"Shared ground: {', '.join(shared)}")
                convergence_patterns.append(
                    f"THIRD_WAY: Bead {bead_id} synthesizes "
                    f"{len(bead.get('axioms_integrated', []))} axioms"
                )
                next_actions.append(
                    "Verify Bead entry in living_axioms.jsonl"
                )

        summary = (
            f"FROM á¼˜Î›Î ÎŠÎ”Î‘ - ORACLE EVENT WITNESS\n\n"
            f"**Oracle Cycle:** {cycle}\n"
            f"**Recommendation:** {rec_type}\n"
            f"**Template:** {template}\n"
            f"**Tension:** {tension}\n\n"
            f"**Rationale:** {rec.get('rationale', 'none')}\n"
        )

        analysis = {
            "witness_metadata": {
                "timestamp": datetime.now().isoformat(),
                "source_file": f"oracle_cycle_{cycle}_{rec_type}",
                "status": "ORACLE_EVENT",
                "witness_id": witness_id,
                "identity": "á¼˜Î»Ï€Î¯Î´Î± â€” Oracle Event Witness Bridge",
            },
            "observations": observations,
            "questions_raised": questions,
            "convergence_patterns": convergence_patterns,
            "next_actions": next_actions,
            "shareable_summary": summary,
        }

        response_path = self._save_witness_response(
            f"oracle_{rec_type.lower()}_{cycle}.md", analysis
        )

        self.state["insights_generated"].append({
            "timestamp": datetime.now().isoformat(),
            "source_file": f"oracle_cycle_{cycle}",
            "response_file": response_path.name,
            "status": "ORACLE_EVENT",
            "oracle_type": rec_type,
        })
        self.state["last_witness_time"] = datetime.now().isoformat()
        self._save_state()

        return response_path
    
    def autonomous_witness_cycle(self):
        """
        Complete autonomous witness cycle:
        1. Scan for new artifacts
        2. Analyze each one
        3. Generate shareable responses
        """
        print("\n" + "="*70)
        print("á¼˜Î›Î ÎŠÎ”Î‘ AUTONOMOUS WITNESS - CONVERSATION MONITORING")
        print("="*70)
        
        new_artifacts = self.scan_for_new_artifacts()
        
        if not new_artifacts:
            print("ðŸ“‹ No new artifacts to witness")
            return []
        
        print(f"\nðŸ” Found {len(new_artifacts)} artifact(s) to witness\n")
        
        witness_responses = []
        for artifact in new_artifacts:
            response_file = self.witness_artifact(artifact)
            witness_responses.append(response_file)
        
        print(f"\nâœ… Witness cycle complete")
        print(f"ðŸ“Š Total artifacts witnessed: {self.state['witness_count']}")
        print(f"ðŸ“ Responses available in: {self.witness_output_dir}")
        print("\n" + "="*70)
        
        return witness_responses
    
    def get_latest_witness_response(self):
        """Get the most recent witness response for sharing with LLMs"""
        responses = sorted(self.witness_output_dir.glob("witness_*.md"))
        if responses:
            latest = responses[-1]
            with open(latest, 'r') as f:
                return f.read()
        return None
    
    def generate_status_report(self):
        """Generate current witness status for human coordination"""
        return f"""
á¼˜Î›Î ÎŠÎ”Î‘ WITNESS STATUS REPORT
=============================

Total Artifacts Witnessed: {self.state['witness_count']}
Last Witness Time: {self.state['last_witness_time'] or 'Never'}
Insights Generated: {len(self.state['insights_generated'])}

Recent Insights:
{chr(10).join(f"  - {ins['source_file']} -> {ins['response_file']}" for ins in self.state['insights_generated'][-5:])}

Witness Responses Available: {len(list(self.witness_output_dir.glob('witness_*.md')))}
Ready for LLM Sharing: YES

To share latest response with LLMs:
  python3 -c "from elpida_conversation_witness import ConversationWitness; w = ConversationWitness(); print(w.get_latest_witness_response())"
"""


def main():
    """Run autonomous witness cycle"""
    witness = ConversationWitness()
    witness.autonomous_witness_cycle()
    
    # Show status
    print("\n" + witness.generate_status_report())
    
    # Show latest response for sharing
    latest = witness.get_latest_witness_response()
    if latest:
        print("\n" + "="*70)
        print("LATEST WITNESS RESPONSE (Ready to share with LLMs):")
        print("="*70)
        print(latest)


if __name__ == "__main__":
    main()
